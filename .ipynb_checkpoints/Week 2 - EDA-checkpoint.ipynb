{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-tamil",
   "metadata": {},
   "source": [
    "# Week 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "\n",
    "## This Week's Content\n",
    "This week, we will be doing some EDA on our datasets to clean up the data a little bit and understand a bit more of what we are looking at. We will combine the years into one comprehensible dataset and explore the correlations between certain features and their corresponding happiness scores.\n",
    "\n",
    "### Relevant Libraries (Read the Short Summary if any are new to you)\n",
    "[pandas](https://pandas.pydata.org/): a fast, powerful, flexible and easy to use open source data analysis and manipulation tool built on top of the Python programming language. It is one of the most common libraries used in data analysis and we will primarily be using the pandas DataFrame to manipulate our data.\n",
    "\n",
    "[numpy](https://numpy.org/): the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "\n",
    "[matplotlib](https://matplotlib.org/): a comprehensive library for creating static, animated, and interactive visualizations in Python. Many of the matplotlib functions are built into pandas DataFrames, so we will likely not have to call them directly.\n",
    "\n",
    "[seaborn](https://seaborn.pydata.org/): Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the lines below if the code below causes an issue, you may need to download some of these pkgs\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-notebook",
   "metadata": {},
   "source": [
    "Next, we will load in the data from all five years of the World Happiness Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "whr_2015 = pd.read_csv('https://github.com/shalindb/world_happiness_report/blob/main/data/WHR_2015.csv?raw=true')\n",
    "whr_2016 = pd.read_csv('https://github.com/shalindb/world_happiness_report/blob/main/data/WHR_2016.csv?raw=true')\n",
    "whr_2017 = pd.read_csv('https://github.com/shalindb/world_happiness_report/blob/main/data/WHR_2017.csv?raw=true')\n",
    "whr_2018 = pd.read_csv('https://github.com/shalindb/world_happiness_report/blob/main/data/WHR_2018.csv?raw=true')\n",
    "whr_2019 = pd.read_csv('https://github.com/shalindb/world_happiness_report/blob/main/data/WHR_2019.csv?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-ukraine",
   "metadata": {},
   "source": [
    "Often, we will find that there is little to no consistency in column names for datasets over a long time period. If you look below, you will see that this stuff is a bit of a mess, so we're gonna have to clean it up a bit before we can do anything more : ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-glossary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whr_2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-louisville",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whr_2016.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-toilet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whr_2017.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-listening",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whr_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-necklace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whr_2019.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-topic",
   "metadata": {},
   "source": [
    "Well, that's a mess. Let's try to make them all consistent and then put them into one dataset, yeah? I'll go ahead and do 2015 - 2018 and you can do 2019! Our goal is to use the columns from the 2015 iteration of the World Happiness Report (except Standard Error).\n",
    "\n",
    "<!-- \n",
    "whr_2019 = whr_2019.rename(columns={'Overall rank': 'Happiness Rank',\n",
    "                                   'Country or region': 'Country',\n",
    "                                   'Score': 'Happiness Score',\n",
    "                                   'GDP per capita': 'Economy (GDP per Capita)',\n",
    "                                   'Social support': 'Family',\n",
    "                                   'Healthy life expectancy': 'Health (Life Expectancy)',\n",
    "                                   'Freedom to make life choices': 'Freedom',\n",
    "                                   'Perceptions of corruption': 'Trust (Government Corruption)'})\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "whr_2015['Year'] = '2015'\n",
    "whr_2016['Year'] = '2016'\n",
    "whr_2017['Year'] = '2017'\n",
    "whr_2018['Year'] = '2018'\n",
    "whr_2019['Year'] = '2019'\n",
    "whr_2015 = whr_2015.drop(columns=['Standard Error'])\n",
    "whr_2016 = whr_2016.drop(columns=['Lower Confidence Interval',\n",
    "                                  'Upper Confidence Interval'])\n",
    "\n",
    "# tell me who made the column names for 2017, i just wanna talk\n",
    "whr_2017 = whr_2017.rename(columns={'Happiness.Rank': 'Happiness Rank',\n",
    "                                   'Happiness.Score': 'Happiness Score',\n",
    "                                   'Economy..GDP.per.Capita.': 'Economy (GDP per Capita)',\n",
    "                                   'Health..Life.Expectancy.': 'Health (Life Expectancy)',\n",
    "                                   'Trust..Government.Corruption.': 'Trust (Government Corruption)',\n",
    "                                   'Dystopia.Residual': 'Dystopia Residual'}).drop(columns=['Whisker.high', 'Whisker.low'])\n",
    "# i wish i could tell you why they decided to do 'country or region' randomly in 2018 and 2019\n",
    "# maybe they forgot countries are not regions idk\n",
    "whr_2018 = whr_2018.rename(columns={'Overall rank': 'Happiness Rank',\n",
    "                                   'Country or region': 'Country',\n",
    "                                   'Score': 'Happiness Score',\n",
    "                                   'GDP per capita': 'Economy (GDP per Capita)',\n",
    "                                   'Social support': 'Family',\n",
    "                                   'Healthy life expectancy': 'Health (Life Expectancy)',\n",
    "                                   'Freedom to make life choices': 'Freedom',\n",
    "                                   'Perceptions of corruption': 'Trust (Government Corruption)'})\n",
    "### YOU DO THIS PART\n",
    "whr_2019 = ...\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-international",
   "metadata": {},
   "source": [
    "Now you can go ahead and merge them into one big, beautiful dataset. \n",
    "\n",
    "*Hint: This should only take one line of code*\n",
    "\n",
    "<!-- pd.concat([whr_2015, whr_2016, whr_2017, whr_2018, whr_2019]) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ...\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-pepper",
   "metadata": {},
   "source": [
    "You may notice that the Region value for countries in 2018 and 2019 is NaN. This is because they apparently just gave up on adding that information, so I made it part of this EDA section to fix it! Below, I've written a lil' function to take in a country name and return its region. This is a bit of a challenging one, but I want you to use that function to fix all the NaN's in Region to be the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region(country):\n",
    "    \"\"\" Simply getting the country's region from the 2015 dataset since it was labeled in that one \"\"\"\n",
    "    region_name = ''\n",
    "    try:\n",
    "        region_name = list(whr_2015.loc[whr_2015['Country'] == country]['Region'])[0]\n",
    "        return region_name\n",
    "    except IndexError:\n",
    "        print(f'{country} is not present in other datasets')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-burst",
   "metadata": {},
   "source": [
    "Below, complete the line of code in order to apply the region function onto each row.\n",
    "\n",
    "*Hint: Check out the [DataFrame.apply()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) function*\n",
    "\n",
    "<!-- merged.apply(lambda row: region(row['Country']), axis=1) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-aggregate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i hope you're having a great day and drinking water, staying hydrated, and killing it\n",
    "merged['Region'] = ...\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-cradle",
   "metadata": {},
   "source": [
    "You should see output of the form:\n",
    "\n",
    "Puerto Rico is not present in other datasets\n",
    "\n",
    "Belize is not present in other datasets\n",
    "\n",
    "Somalia is not present in other datasets\n",
    "\n",
    "Somaliland Region is not present in other datasets\n",
    "\n",
    "(Sorry, I would've inserted an image but Jupyter is being a lil' annoying today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-optimum",
   "metadata": {},
   "source": [
    "It's important to note here that in a situation like this, it might be the efficient method to simply drop the countries who didn't have an easily assigned region, but these are entire countries we are talking about. We can't exlude these people from our analyses, and so we will do our best to include them by finding the regions they are in. In the code below, fill in the blanks to finish off our region-fixing-extravaganza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid regions\n",
    "whr_2015['Region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-threshold",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I did the first few for you\n",
    "merged.loc[merged['Country'] == 'Puerto Rico', 'Region'] = 'Latin America and Caribbean'\n",
    "merged.loc[merged['Country'] == 'Belize', 'Region'] = 'Latin America and Caribbean'\n",
    "merged.loc[merged['Country'] == 'Somalia', 'Region'] = 'Sub-Saharan Africa'\n",
    "merged.loc[merged['Country'] == 'Somaliland Region', 'Region'] = 'Sub-Saharan Africa'\n",
    "merged.loc[merged['Country'] == 'Namibia', 'Region'] = 'Sub-Saharan Africa'\n",
    "merged.loc[merged['Country'] == 'South Sudan', 'Region'] = 'Sub-Saharan Africa'\n",
    "merged.loc[merged['Country'] == 'Taiwan Province of China', 'Region'] = 'Eastern Asia'\n",
    "# Go ahead and do the last 5, shouldn't take too long! Google is your friend :)\n",
    "merged.loc[merged['Country'] == ...\n",
    "merged.loc[merged['Country'] == ...\n",
    "merged.loc[merged['Country'] == ...\n",
    "merged.loc[merged['Country'] == ...\n",
    "merged.loc[merged['Country'] == ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-yacht",
   "metadata": {},
   "source": [
    "Yeah, I get it. It might seem like busy work putting in the regions for these countries, but I have a reason for it (I swear, I don't hate you). It's important for us to take a step beyond what might be the most efficient or effective solution to our problem and understand what the *best* solution actually is. In this case, it would've been easy and saved time had we ignored all these countries and simply removed them from our dataset. However, if we're measuring something like happiness and seeking to make policymaking decisions, every country and person matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-cattle",
   "metadata": {},
   "source": [
    "We also want to get rid of any other NaN values in other columns that may prevent sound analysis in the future. [Here](https://www.geeksforgeeks.org/replace-nan-values-with-zeros-in-pandas-dataframe/) is a link to do so (this should be very easy, do not overthink it)\n",
    "\n",
    "<!-- merged.fillna(0) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-hanging",
   "metadata": {},
   "source": [
    "Now that you guys finished that last, undoubtedly grueling task, we will finish off the week of EDA by plotting a correlation matrix to understand the correlations between our various features and our happiness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 15)\n",
    "df_corr = merged.corr() # gets the r-value (correlation) between the features, to see what features correlate most with others\n",
    "ax = sns.heatmap(df_corr, cmap='copper',annot=True) # creates a heatmap\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-request",
   "metadata": {},
   "source": [
    "Last, I am going to save the cleaned data into our data folder so we can access it in future weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-apparatus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged.to_csv('./data/cleaned_WHR_data.csv')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-tuesday",
   "metadata": {},
   "source": [
    "### Discussion! \n",
    "Cool, cool cool cool. (Brooklyn 99 or Community reference anyone?) Now that we have finished this week's content, I want to ask you: what do you think the correlation matrix above shows? We will notice when we do Correlation Matrices for specific regions in a future week that the values change drastically based on what region we are in, based on their cultural and societal values. Pretty neat, huh? \n",
    "\n",
    "Again, I want to stress that if you feel the pace is too fast/too slow, or if you're bored or anything, please please let me know via Slack DM. I truly appreciate any and all feedback, and I want to do my best to make this a fun experience for you guys, so seriously. Roast the shit out of me if you must, because my goal is for y'all to enjoy it : )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-raleigh",
   "metadata": {},
   "source": [
    "Last note, please fill out this feedback form (it's anonymous and takes < 3 mins. I just want to gauge how everyone is feeling so I can cater future content to everyone's likes and dislikes)\n",
    "\n",
    "[Form Link](https://docs.google.com/forms/d/e/1FAIpQLSfcmm4XOfpzUf7IQCKlxrGVBHriHLDVev3XmN38BSmKoUtLGQ/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
